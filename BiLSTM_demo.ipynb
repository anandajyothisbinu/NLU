{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import keras_tuner as kt\n",
    "import gensim.downloader as api\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ED/test.csv') # change to filepath for predictions\n",
    "train_df = pd.read_csv('./ED/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = train_df['Claim']\n",
    "evidence = train_df['Evidence']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(claims)\n",
    "tokenizer.fit_on_texts(evidence)\n",
    "\n",
    "claims_seq = tokenizer.texts_to_sequences(claims)\n",
    "evidence_seq = tokenizer.texts_to_sequences(evidence)\n",
    "\n",
    "CLAIMS_LEN = 13\n",
    "EVIDENCE_LEN = 65\n",
    "claims_seq = pad_sequences(claims_seq, maxlen=EVIDENCE_LEN, padding='post')\n",
    "evidence_seq = pad_sequences(evidence_seq, maxlen=EVIDENCE_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = glove_vectors.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = df['Claim']\n",
    "evidence = df['Evidence'].fillna('')\n",
    "\n",
    "claims_seq = tokenizer.texts_to_sequences(claims)\n",
    "evidence_seq = tokenizer.texts_to_sequences(evidence)\n",
    "\n",
    "claims_seq = pad_sequences(claims_seq, maxlen=EVIDENCE_LEN, padding='post')\n",
    "evidence_seq = pad_sequences(evidence_seq, maxlen=EVIDENCE_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    claims = Input(shape=(EVIDENCE_LEN,))\n",
    "    evidence = Input(shape=(EVIDENCE_LEN,))\n",
    "\n",
    "    shared_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=EVIDENCE_LEN, trainable=False)\n",
    "\n",
    "    x1 = shared_embedding(claims)\n",
    "    x2 = shared_embedding(evidence)\n",
    "    for i in range(hp.Int('num_bilstm_layers', 1, 3)):\n",
    "        dropout = hp.Float(f'dropout_{i}', min_value=0, max_value=0.5, step=0.1)\n",
    "        x1 = Bidirectional(LSTM(units=hp.Choice(f'lstm_units_{i}', values=[32, 64, 128, 256]),\n",
    "                                dropout=dropout,\n",
    "                                recurrent_dropout=dropout,\n",
    "                                return_sequences=True if i < hp.get('num_bilstm_layers') - 1 else False))(x1)\n",
    "        x2 = Bidirectional(LSTM(units=hp.Choice(f'lstm_units_{i}', values=[32, 64, 128, 256]),\n",
    "                                dropout=dropout,\n",
    "                                recurrent_dropout=dropout,\n",
    "                                return_sequences=True if i < hp.get('num_bilstm_layers') - 1 else False))(x2)\n",
    "\n",
    "    combined = Concatenate()([x1, x2])\n",
    "\n",
    "    for k in range(hp.Int('num_dense_layers', 1, 3)):\n",
    "        combined = Dense(units=hp.Choice(f'dense_units_{k}', values=[32, 64, 128, 256]),\n",
    "                         activation='relu')(combined)\n",
    "        combined = Dropout(rate=hp.Float(f'dense_dropout_{k}', min_value=0, max_value=0.5, step=0.1))(combined)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = Model(inputs=[claims, evidence], outputs=output)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 81 variables. \n"
     ]
    }
   ],
   "source": [
    "with open('best_hyperparameters.json', 'r') as f:\n",
    "    loaded_hyperparams = json.load(f)\n",
    "\n",
    "hyperparams = kt.HyperParameters()\n",
    "for key, value in loaded_hyperparams.items():\n",
    "    hyperparams.Fixed(key, value)\n",
    "\n",
    "model = build_model(hyperparams)\n",
    "model.load_weights('best.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 11s 32ms/step - loss: 0.3696 - accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([claims_seq, evidence_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.round()\n",
    "predictions = predictions.astype(int)\n",
    "predictions = predictions.flatten()\n",
    "predictions = pd.DataFrame(predictions, columns=['prediction'])\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
